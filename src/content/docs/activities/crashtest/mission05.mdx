---
title: Mission 5 - Haute disponibilit√© de GLPI
description: Mise en place d'une architecture haute disponibilit√© avec load balancing et r√©plication
draft: true
---

import { Steps, Card, CardGrid, Aside } from '@astrojs/starlight/components';

# Mission 5 : Haute disponibilit√© de GLPI

## Objectif

Mettre en place une architecture haute disponibilit√© pour garantir la continuit√© de service du portail GLPI m√™me en cas de panne d'un serveur.

<CardGrid>
  <Card title="Difficult√©" icon="star">
    Difficile üí™
  </Card>
  <Card title="Temps estim√©" icon="clock">
    8 heures
  </Card>
</CardGrid>

## Pr√©requis

- Avoir termin√© les Missions 1, 2 et 3
- Ma√Ætriser l'administration Linux avanc√©e
- Comprendre les concepts de r√©plication de bases de donn√©es
- Conna√Ætre les principes du load balancing

## Contexte

Actuellement, le portail GLPI du CRASH repose sur une architecture avec des **points de d√©faillance uniques** (_Single Point of Failure_ - SPOF) :
- Un seul serveur web : si celui-ci tombe, le service est indisponible
- Une seule base de donn√©es : en cas de panne, plus aucune op√©ration possible
- Aucun m√©canisme de basculement automatique

Le CRASH a connu plusieurs incidents ces derniers mois :
- Panne mat√©rielle d'un serveur ‚Üí 4 heures d'indisponibilit√©
- Mise √† jour syst√®me mal pr√©par√©e ‚Üí 2 heures d'interruption
- Saturation disque ‚Üí Service d√©grad√© pendant 6 heures

**Les associations sportives d√©pendent du portail GLPI pour :**
- D√©clarer des incidents urgents (panne r√©seau, probl√®me d'acc√®s)
- Suivre leurs demandes de support
- Consulter la base de connaissances
- G√©rer leur inventaire informatique

Une indisponibilit√© prolong√©e perturbe gravement leur activit√©.

:::caution[Cahier des charges haute disponibilit√©]
**Objectifs de disponibilit√© :**
- **RTO** (_Recovery Time Objective_) : Temps de reprise < 2 minutes
- **RPO** (_Recovery Point Objective_) : Perte de donn√©es < 1 minute
- **Disponibilit√© cible** : 99,9% (‚âà 8h d'indisponibilit√© par an maximum)

**Contraintes techniques :**
- Budget limit√© : utilisation de solutions open source
- Infrastructure existante : VirtualBox, serveurs Debian/Ubuntu
- R√©seau : DMZ (10.54.0.0/24) et LAN (172.17.0.0/16)
:::

:::tip[Approche p√©dagogique]
Cette mission propose trois niveaux de r√©alisation :
- ‚≠ê Architecture HA de base avec load balancing web
- ‚≠ê‚≠ê R√©plication de base de donn√©es et √©limination des SPOF critiques
- ‚≠ê‚≠ê‚≠ê Architecture production-ready avec monitoring et automatisation

Chaque niveau inclut le pr√©c√©dent. Visez au minimum le niveau ‚≠ê pour valider la mission.
:::

---

## Concepts de haute disponibilit√©

### Composants d'une architecture HA

#### Load Balancer (r√©partiteur de charge)

Distribue le trafic entre plusieurs serveurs backend.

**Technologies disponibles :**
- **HAProxy** : Load balancer TCP/HTTP mature et performant
- **Nginx** : Serveur web avec fonctions de load balancing
- **Traefik** : Load balancer moderne pour conteneurs

**Algorithmes de r√©partition :**
- **Round-robin** : Distribution circulaire √©quitable
- **Least connections** : Vers le serveur le moins charg√©
- **IP hash** : M√™me client ‚Üí m√™me serveur (affinit√© de session)

#### IP virtuelle flottante (VRRP)

Adresse IP qui ¬´ flotte ¬ª entre plusieurs machines gr√¢ce au protocole VRRP (_Virtual Router Redundancy Protocol_).

**Principe :**
- Deux serveurs partagent une IP virtuelle (VIP)
- Un seul est ¬´ ma√Ætre ¬ª √† la fois et poss√®de la VIP
- Si le ma√Ætre tombe, le backup prend automatiquement la VIP
- Transparent pour les clients (m√™me IP, pas de coupure)

**Technologies disponibles :**
- **Keepalived** : Impl√©mentation VRRP simple et fiable
- **Corosync/Pacemaker** : Solution clustering avanc√©e (complexe)

#### R√©plication de base de donn√©es

Copie automatique des donn√©es d'une base vers une ou plusieurs autres.

**Topologies MySQL/MariaDB :**

**Master-Slave (r√©plication unidirectionnelle) :**
- Un serveur ma√Ætre (√©criture)
- Un ou plusieurs esclaves (lecture seule)
- R√©plication asynchrone des modifications

**Master-Master (r√©plication bidirectionnelle) :**
- Deux serveurs ma√Ætres (√©criture sur les deux)
- R√©plication mutuelle
- Plus complexe (gestion des conflits)

#### Synchronisation des fichiers

Les fichiers upload√©s dans GLPI (documents, images) doivent √™tre accessibles par tous les serveurs web.

**Solutions possibles :**

**NFS (Network File System) :**
- Partage de r√©pertoire r√©seau
- Simple √† mettre en place
- ‚ö†Ô∏è Constitue un SPOF si le serveur NFS tombe

**GlusterFS :**
- Syst√®me de fichiers distribu√© et r√©pliqu√©
- Pas de SPOF
- Plus complexe √† configurer

**Lsyncd (Live Syncing Daemon) :**
- Synchronisation temps r√©el via rsync
- Simple et l√©ger
- Unidirectionnel (master ‚Üí slaves)

---

## Architecture cible

### Architecture par niveau

#### ‚≠ê Niveau 1 : Load balancing web de base
```
                    [Utilisateurs]
                          |
                     [HAProxy]
                    /          \
           [glpi-web01]    [glpi-web02]
                    \          /
                  [glpi-db01] (SPOF)
                         |
              [NFS ou synchronisation] (SPOF)
```

**Gains :**
- ‚úÖ Redondance des serveurs web
- ‚úÖ R√©partition de charge
- ‚ùå Base de donn√©es = SPOF
- ‚ùå Stockage fichiers = SPOF

---

#### ‚≠ê‚≠ê Niveau 2 : √âlimination des SPOF critiques
```
                    [Utilisateurs]
                          |
                 [IP Virtuelle VRRP]
                    /          \
          [haproxy01]      [haproxy02]
           (master)         (backup)
                |                |
        +-------+----------------+-------+
        |                                |
   [glpi-web01]                    [glpi-web02]
        |                                |
        +-------+----------------+-------+
                |                |
          [glpi-db01]      [glpi-db02]
           (master)         (slave)
        Master-Slave replication
```

**Gains :**
- ‚úÖ Redondance HAProxy avec IP flottante
- ‚úÖ R√©plication base de donn√©es
- ‚úÖ Basculement automatique si panne
- ‚ö†Ô∏è Stockage peut rester SPOF (acceptable)

---

#### ‚≠ê‚≠ê‚≠ê Niveau 3 : Architecture production-ready
```
                    [Utilisateurs]
                          |
                 [IP Virtuelle VRRP]
                    /          \
          [haproxy01]      [haproxy02]
           + Keepalived   + Keepalived
                |                |
        +-------+--------+-------+-------+
        |                |               |
   [glpi-web01]    [glpi-web02]    [glpi-web03]
        |                |               |
        +-------+--------+-------+-------+
                         |
                    [GlusterFS]
                  r√©plication 3 voies
                         |
        +-------+--------+-------+
        |                        |
   [glpi-db01]            [glpi-db02]
    (master)               (master)
      Master-Master replication
```

**Gains :**
- ‚úÖ Aucun SPOF
- ‚úÖ 3+ serveurs web
- ‚úÖ Stockage r√©pliqu√© (GlusterFS)
- ‚úÖ BDD Master-Master
- ‚úÖ Monitoring (Prometheus/Grafana)
- ‚úÖ Automatisation compl√®te

---

## Plan d'adressage recommand√©

:::tip[Conventions de nommage]
Utilisez des noms coh√©rents pour faciliter la gestion :

**Load balancers :**
- `haproxy01`, `haproxy02`

**Serveurs web :**
- `glpi-web01`, `glpi-web02`, `glpi-web03`

**Bases de donn√©es :**
- `glpi-db01`, `glpi-db02`

**Services partag√©s :**
- `glpi-nfs01` (si NFS)
- `glpi-gluster01`, `glpi-gluster02`, `glpi-gluster03` (si GlusterFS)
:::

**Tableau d'adressage √† compl√©ter :**

| √âquipement | Zone | Adresse IP | R√¥le |
|------------|------|------------|------|
| VIP (IP virtuelle) | DMZ | | IP publique du cluster |
| haproxy01 | DMZ | | Load balancer principal |
| haproxy02 | DMZ | | Load balancer backup |
| glpi-web01 | DMZ | | Serveur web 1 |
| glpi-web02 | DMZ | | Serveur web 2 |
| glpi-web03 | DMZ | | Serveur web 3 (niveau 3) |
| glpi-db01 | DMZ | | Base de donn√©es master |
| glpi-db02 | DMZ | | Base de donn√©es slave/master |
| glpi-nfs01 | DMZ | | Serveur NFS (si utilis√©) |

---

## T√¢ches √† r√©aliser

### T√¢che 1 ‚≠ê : Analyse et justification de l'architecture

Avant de d√©ployer, analysez les besoins et choisissez vos technologies.

#### √âtape 1.1 : Identification des SPOF actuels

**Analysez votre infrastructure actuelle** (issue de la Mission 1) et identifiez tous les points de d√©faillance uniques.

Pour chaque composant, r√©pondez :
- Que se passe-t-il si ce composant tombe ?
- Quel est l'impact m√©tier ?
- Quelle est la probabilit√© de panne ?

Cr√©ez un tableau d'analyse des risques :

| Composant | Impact si panne | Probabilit√© | Niveau de risque | Mitigation pr√©vue |
|-----------|----------------|-------------|------------------|-------------------|
| Serveur web unique | | | | |
| Base de donn√©es unique | | | | |
| Stockage fichiers | | | | |
| Load balancer | | | | |

#### √âtape 1.2 : Choix des technologies

Pour chaque brique de votre architecture HA, justifiez votre choix technologique :

**Load balancer :**
- HAProxy, Nginx ou Traefik ?
- Pourquoi ?

**IP virtuelle :**
- Keepalived, Corosync/Pacemaker ou autre ?
- Pourquoi ?

**R√©plication BDD :**
- Master-Slave ou Master-Master ?
- MySQL natif ou MariaDB ?
- Pourquoi ?

**Synchronisation fichiers :**
- NFS, GlusterFS, Lsyncd ou autre ?
- Pourquoi ?

:::note[Crit√®res de choix]
Consid√©rez :
- La **simplicit√©** de mise en ≈ìuvre (temps disponible)
- La **fiabilit√©** et maturit√© de la solution
- Vos **comp√©tences** et exp√©rience
- La **documentation** disponible
- La compatibilit√© avec votre infrastructure existante
:::

---

### T√¢che 2 ‚≠ê : Pr√©paration de l'infrastructure

Cr√©ez les machines virtuelles n√©cessaires √† votre architecture.

#### Cr√©ation des VMs

**Niveau ‚≠ê minimum :**
- 1 VM HAProxy : `haproxy01`
- 1 VM serveur web suppl√©mentaire : `glpi-web02`
- R√©utilisation de `glpi-srv01` ou `glpi-web01` (Mission 1)

**Niveau ‚≠ê‚≠ê :**
- Ajoutez : `haproxy02`, `glpi-db02`

**Niveau ‚≠ê‚≠ê‚≠ê :**
- Ajoutez : `glpi-web03`, nodes GlusterFS si n√©cessaire

#### Configuration r√©seau

Toutes les machines du cluster doivent √™tre dans la **DMZ** (10.54.0.0/24).

Configurez les interfaces r√©seau selon votre plan d'adressage.

#### Synchronisation temporelle

**Critique pour la r√©plication** : tous les serveurs doivent avoir l'heure exacte.

Configurez NTP sur toutes les machines :
- M√™me serveur NTP pour tout le cluster
- V√©rifiez la synchronisation avec `ntpdate -q` ou `timedatectl`

---

### T√¢che 3 ‚≠ê : Clonage et configuration des serveurs web

Vous devez avoir au moins **2 serveurs web identiques** pour le load balancing.

#### Clonage du serveur existant

Si vous avez d√©j√† `glpi-web01` (ou `glpi-srv01`) fonctionnel depuis la Mission 1 :

**Option 1 : Clonage de VM (VirtualBox)**
- Clonez la VM existante pour cr√©er `glpi-web02`
- R√©g√©n√©rez les identifiants machine (UUID, cl√©s SSH, etc.)
- Modifiez le hostname et l'adresse IP

**Option 2 : Automatisation (niveau ‚≠ê‚≠ê‚≠ê)**
- Utilisez votre script d'installation automatis√© (Mission 1 ‚≠ê‚≠ê‚≠ê)
- D√©ployez `glpi-web02` et `glpi-web03` automatiquement

#### Configuration GLPI pour environnement clusteris√©

**D√©sactivez les t√¢ches cron GLPI** sur les serveurs web secondaires :
- Les t√¢ches automatiques (notifications, mises √† jour) ne doivent s'ex√©cuter que sur **un seul** serveur
- Sur `glpi-web02` (et web03), d√©sactivez le cron GLPI

**Configurez les sessions PHP** :
- Par d√©faut, PHP stocke les sessions en fichiers locaux ‚Üí probl√®me en cluster
- Solution 1 : Sessions en base de donn√©es (recommand√© pour GLPI)
- Solution 2 : Memcached ou Redis partag√© (plus performant)

:::tip[Sessions GLPI en base de donn√©es]
Dans le fichier de configuration GLPI, activez le stockage des sessions en BDD :
```php
// config/config_db.php
$CFG_GLPI['session_use_db'] = true;
```
Ainsi, un utilisateur peut changer de serveur web sans perdre sa session.
:::

---

### T√¢che 4 ‚≠ê : Installation et configuration de HAProxy

Installez HAProxy pour r√©partir le trafic entre vos serveurs web.

#### Installation de HAProxy

Sur la VM `haproxy01`, installez HAProxy.

#### Configuration de base

Cr√©ez une configuration HAProxy (`/etc/haproxy/haproxy.cfg`) qui :

**Frontend (√©coute des connexions entrantes) :**
- √âcoute sur le port 443 (HTTPS)
- √âcoute sur le port 80 (HTTP) avec redirection vers HTTPS

**Backend (serveurs web GLPI) :**
- Liste des serveurs : `glpi-web01`, `glpi-web02`
- Algorithme de load balancing √† choisir (round-robin, least-conn, etc.)
- Health checks HTTP (v√©rification que les serveurs r√©pondent)

**Options importantes :**
- Persistance de session (stick-table ou cookie)
- D√©tection automatique des pannes (health checks)
- Timeouts adapt√©s

:::caution[Certificats SSL/TLS]
Pour le HTTPS, HAProxy a besoin des certificats SSL :
- R√©utilisez les certificats de la Mission 1 (auto-sign√©s)
- Ou g√©n√©rez de nouveaux certificats pour le cluster
- HAProxy peut faire la terminaison SSL (clients ‚Üí HAProxy en HTTPS, HAProxy ‚Üí backends en HTTP)
:::

#### Tests de fonctionnement

**Testez le load balancing :**

1. Acc√©dez √† GLPI via l'IP de HAProxy
2. V√©rifiez que les requ√™tes sont bien r√©parties entre glpi-web01 et glpi-web02
3. Arr√™tez `glpi-web01` et v√©rifiez que le service reste disponible via `glpi-web02`
4. Red√©marrez `glpi-web01` et v√©rifiez qu'il reprend automatiquement du trafic

_Astuce :_ Consultez les statistiques HAProxy via l'interface web (si activ√©e) ou les logs.

---

### T√¢che 5 ‚≠ê‚≠ê : Mise en place de l'IP virtuelle avec Keepalived

Cr√©ez une IP virtuelle flottante pour √©liminer HAProxy comme SPOF.

#### Principe de fonctionnement

- Deux HAProxy : `haproxy01` (master) et `haproxy02` (backup)
- Une seule IP virtuelle (VIP) partag√©e
- Keepalived g√®re le protocole VRRP :
  - `haproxy01` poss√®de la VIP en temps normal
  - Si `haproxy01` tombe, `haproxy02` prend automatiquement la VIP
  - Basculement en quelques secondes, transparent pour les clients

#### Installation et configuration de Keepalived

Sur **les deux** HAProxy (`haproxy01` et `haproxy02`) :

1. Installez Keepalived
2. Cr√©ez la configuration VRRP :
   - D√©finissez l'IP virtuelle
   - Priorit√© : `haproxy01` (ex: 100) > `haproxy02` (ex: 90)
   - Intervalle de v√©rification (advertisement)
   - Script de v√©rification de l'√©tat de HAProxy

**Configuration √† adapter :**
- Interface r√©seau
- Adresse IP virtuelle (VIP)
- Authentification VRRP (mot de passe partag√©)

#### Mise √† jour du pare-feu

Modifiez les r√®gles Stormshield (Mission 2) :
- Le NAT entrant (DNAT) doit pointer vers la **VIP** et non plus vers une IP fixe
- Autorisez le protocole VRRP (protocole 112) entre `haproxy01` et `haproxy02`

#### Tests de basculement

**Testez le failover automatique :**

1. Identifiez quel HAProxy poss√®de actuellement la VIP (commande `ip addr`)
2. Acc√©dez √† GLPI via la VIP
3. **Arr√™tez brutalement le HAProxy master** (simuler une panne)
```bash
   sudo systemctl stop haproxy
   # ou plus brutal : sudo poweroff
```
4. V√©rifiez que :
   - Le backup prend la VIP en moins de 5 secondes
   - Le service GLPI reste accessible via la VIP (br√®ve coupure acceptable)
   - Les utilisateurs connect√©s ne perdent pas leur session
5. Red√©marrez le master et v√©rifiez qu'il reprend la VIP

**Documentez** le temps de basculement observ√© et les √©ventuelles interruptions.

---

### T√¢che 6 ‚≠ê‚≠ê : R√©plication de la base de donn√©es MySQL/MariaDB

Mettez en place la r√©plication Master-Slave pour s√©curiser les donn√©es.

#### Configuration du serveur Master (glpi-db01)

**Activez la r√©plication :**
- Configurez MySQL/MariaDB pour √©crire les binlogs
- D√©finissez un `server-id` unique
- Cr√©ez un utilisateur de r√©plication avec les privil√®ges appropri√©s

**Configuration √† modifier** (g√©n√©ralement dans `/etc/mysql/mariadb.conf.d/50-server.cnf` ou √©quivalent) :
```ini
[mysqld]
server-id = 1
log_bin = /var/log/mysql/mysql-bin.log
binlog_do_db = glpidb
```

#### Configuration du serveur Slave (glpi-db02)

**Configurez le Slave :**
- `server-id` diff√©rent du master (ex: 2)
- Pointez vers le master avec les identifiants de r√©plication
- D√©marrez la r√©plication

**V√©rifiez l'√©tat de la r√©plication :**
```sql
SHOW SLAVE STATUS\G
```

Surveillez particuli√®rement :
- `Slave_IO_Running: Yes`
- `Slave_SQL_Running: Yes`
- `Seconds_Behind_Master: 0` (ou valeur faible)

#### Tests de r√©plication

1. **Sur le master** : Cr√©ez un ticket de test dans GLPI
2. **Sur le slave** : V√©rifiez que le ticket appara√Æt (quasi instantan√©ment)
3. **Simulez une panne du master** : Arr√™tez `glpi-db01`
4. **Promouvez le slave en master** (processus manuel pour niveau ‚≠ê‚≠ê)

:::caution[Limitation Master-Slave]
En r√©plication Master-Slave :
- Le slave est **lecture seule** (read-only)
- Si le master tombe, vous devez **manuellement** :
  1. Promouvoir le slave en master
  2. Reconfigurer les serveurs web pour pointer vers le nouveau master
  
Ce n'est pas un failover automatique. Pour cela, il faudrait du Master-Master (niveau ‚≠ê‚≠ê‚≠ê) ou un outil d'orchestration.
:::

---

### T√¢che 7 ‚≠ê‚≠ê : Synchronisation des fichiers GLPI

Les fichiers upload√©s dans GLPI (documents, images de tickets, etc.) doivent √™tre accessibles par tous les serveurs web.

#### Solution 1 : NFS (simple, SPOF acceptable)

**Sur un serveur NFS d√©di√©** (`glpi-nfs01`) ou sur `glpi-db01` :

1. Installez le serveur NFS
2. Exportez le r√©pertoire des fichiers GLPI (g√©n√©ralement `/var/www/html/glpi/files`)
3. Configurez les permissions et les ACL

**Sur chaque serveur web** (`glpi-web01`, `glpi-web02`) :

1. Installez le client NFS
2. Montez le partage NFS sur `/var/www/html/glpi/files`
3. Configurez le montage automatique au boot (`/etc/fstab`)

**Testez** :
- Uploadez un document dans GLPI via `glpi-web01`
- V√©rifiez qu'il est accessible en passant par `glpi-web02`

#### Solution 2 : Lsyncd (synchronisation temps r√©el)

Alternative sans serveur NFS central (√©vite un SPOF) :

1. Installez Lsyncd sur `glpi-web01` (source)
2. Configurez la synchronisation vers `glpi-web02` et `glpi-web03` (cibles)
3. Utilisez rsync + SSH pour le transfert
4. Lsyncd surveille les modifications et les propage automatiquement

**Limitation** : Unidirectionnel. Si un utilisateur upload via `glpi-web02`, le fichier doit √™tre synchronis√© vers `glpi-web01` puis vers les autres (topologie en √©toile).

#### Solution 3 : GlusterFS (niveau ‚≠ê‚≠ê‚≠ê)

Pour une vraie r√©plication sans SPOF :

1. Installez GlusterFS sur 3 n≈ìuds minimum
2. Cr√©ez un volume r√©pliqu√© (replica 3)
3. Montez le volume GlusterFS sur tous les serveurs web
4. Les fichiers sont automatiquement r√©pliqu√©s sur les 3 n≈ìuds

**Avantage** : Aucun SPOF, haute disponibilit√© compl√®te du stockage.  
**Inconv√©nient** : Plus complexe √† configurer et maintenir.

---

### T√¢che 8 ‚≠ê‚≠ê‚≠ê : R√©plication Master-Master MySQL

Passez √† une r√©plication bidirectionnelle pour permettre l'√©criture sur les deux serveurs BDD.

#### Configuration Master-Master

**Principe** :
- `glpi-db01` et `glpi-db02` sont tous deux masters
- Chaque serveur r√©plique vers l'autre
- Les deux peuvent recevoir des √©critures

**Configuration √† r√©aliser** :
- Sur `glpi-db01` : Configurer la r√©plication vers `glpi-db02`
- Sur `glpi-db02` : Configurer la r√©plication vers `glpi-db01`
- Activer l'auto-increment offset pour √©viter les conflits d'ID :
```ini
  # glpi-db01
  auto_increment_offset = 1
  auto_increment_increment = 2
  
  # glpi-db02
  auto_increment_offset = 2
  auto_increment_increment = 2
```

:::caution[Gestion des conflits]
En Master-Master, des conflits peuvent survenir si :
- M√™me ligne modifi√©e simultan√©ment sur les deux masters
- Contraintes d'unicit√© viol√©es

**Bonnes pratiques** :
- Utilisez un load balancer pour la BDD (ProxySQL) qui dirige les √©critures vers un seul master √† la fois
- Surveillez les logs de r√©plication (`SHOW SLAVE STATUS`)
- Testez les sc√©narios de bascule
:::

#### Configuration HAProxy pour la base de donn√©es

Ajoutez un frontend/backend dans HAProxy pour la base de donn√©es :
- Frontend sur port 3306
- Backend : `glpi-db01` (master actif) et `glpi-db02` (backup)
- Health check MySQL

Les serveurs web pointent vers HAProxy (IP:3306) plut√¥t que directement vers un serveur BDD.

---

### T√¢che 9 ‚≠ê‚≠ê‚≠ê : Monitoring et supervision

Mettez en place une surveillance compl√®te du cluster.

#### Monitoring avec Prometheus + Grafana

**D√©ployez la stack de monitoring :**

1. **Prometheus** : Collecte des m√©triques
   - Installez Prometheus (VM d√©di√©e ou sur un n≈ìud existant)
   - Configurez les targets (HAProxy, serveurs web, BDD)
   - Installez les exporters :
     - `node_exporter` sur chaque VM (m√©triques syst√®me)
     - `mysqld_exporter` sur les serveurs BDD
     - `haproxy_exporter` ou utilisez les stats HAProxy

2. **Grafana** : Visualisation
   - Installez Grafana
   - Connectez-le √† Prometheus (source de donn√©es)
   - Importez des dashboards pr√©-faits ou cr√©ez les v√¥tres

**M√©triques importantes √† surveiller :**

**Serveurs web :**
- CPU, RAM, disque
- Nombre de requ√™tes HTTP/s
- Temps de r√©ponse
- Erreurs 5xx

**Base de donn√©es :**
- √âtat de la r√©plication (lag, erreurs)
- Nombre de connexions actives
- Requ√™tes lentes
- Utilisation disque

**HAProxy :**
- Nombre de connexions actives
- Sant√© des backends (up/down)
- R√©partition du trafic
- Erreurs de connexion

#### Alertes automatiques

Configurez des alertes (Prometheus Alertmanager ou Grafana Alerts) :
- Serveur web down
- R√©plication MySQL cass√©e ou en retard
- HAProxy backend down
- Disque > 80% plein
- CPU > 90% pendant 5 minutes

**Canaux de notification :**
- Email
- Slack/Discord/Mattermost (webhook)
- SMS (via service externe)

---

### T√¢che 10 ‚≠ê‚≠ê‚≠ê : Tests de charge et sc√©narios de panne

Validez la robustesse de votre architecture par des tests approfondis.

#### Test 1 : Charge normale et r√©partition

**Objectif** : V√©rifier que HAProxy r√©partit bien la charge.

Utilisez un outil de test de charge :
```bash
# Avec Apache Bench
ab -n 10000 -c 100 https://[VIP]/

# Avec wrk (plus moderne)
wrk -t4 -c100 -d30s https://[VIP]/
```

**V√©rifiez** :
- Le trafic est r√©parti entre les serveurs web
- Aucune erreur 5xx
- Temps de r√©ponse acceptable
- Pas de saturation CPU/RAM

#### Test 2 : Panne d'un serveur web

**Sc√©nario** : Un serveur web tombe en production.

1. Lancez un test de charge continu
2. **Arr√™tez brutalement `glpi-web01`** (simuler crash mat√©riel)
3. Observez :
   - HAProxy d√©tecte la panne (health check)
   - Le trafic est redirig√© vers `glpi-web02`
   - Les utilisateurs connect√©s ne sont pas d√©connect√©s (sessions en BDD)
   - Nombre d'erreurs minimal (acceptable pendant la d√©tection de panne)
4. Red√©marrez `glpi-web01` et v√©rifiez qu'il reprend du trafic automatiquement

**Documentez** :
- Temps de d√©tection de la panne
- Nombre de requ√™tes √©chou√©es
- Comportement des sessions utilisateur

#### Test 3 : Panne du HAProxy master

**Sc√©nario** : Le load balancer principal tombe.

1. Identifiez le HAProxy master (qui poss√®de la VIP)
2. Acc√©dez √† GLPI via la VIP
3. **Arr√™tez le HAProxy master**
4. Observez :
   - Keepalived d√©tecte la panne (VRRP)
   - Le backup prend la VIP
   - Temps de bascule (devrait √™tre < 5 secondes)
   - Le service reste accessible avec une br√®ve interruption

**Documentez** :
- Temps de basculement observ√©
- Ressenti utilisateur (coupure perceptible ?)

#### Test 4 : Panne de la base de donn√©es master

**Sc√©nario** : Le serveur BDD master tombe.

**En Master-Slave (niveau ‚≠ê‚≠ê) :**

1. **Arr√™tez `glpi-db01`** (master)
2. **Proc√©dure manuelle** :
   - Promouvoir `glpi-db02` en master
   - Reconfigurer les serveurs web ou HAProxy pour pointer vers `glpi-db02`
   - Tester que GLPI fonctionne en lecture/√©criture
3. **Documentez** le temps d'intervention et les √©tapes

**En Master-Master (niveau ‚≠ê‚≠ê‚≠ê) :**

1. **Arr√™tez `glpi-db01`**
2. HAProxy (frontend BDD) d√©tecte la panne et bascule vers `glpi-db02`
3. Le service reste disponible automatiquement
4. **Documentez** le temps de bascule automatique

#### Test 5 : Panne simultan√©e (test ultime)

**Sc√©nario** : Plusieurs composants tombent en m√™me temps (panne √©lectrique, probl√®me r√©seau...).

**Exemple** : `glpi-web01` + `haproxy01` tombent simultan√©ment.

1. Arr√™tez les deux VMs en m√™me temps
2. Observez :
   - HAProxy backup prend la VIP
   - `glpi-web02` g√®re tout le trafic web
   - Le service reste accessible (peut-√™tre d√©grad√© si charge √©lev√©e)
3. **Documentez** la r√©silience du syst√®me

#### Rapport de tests

R√©digez un **rapport de tests de r√©silience** incluant :

1. **M√©thodologie** : Sc√©narios test√©s, outils utilis√©s
2. **R√©sultats** : Temps de bascule, erreurs observ√©es, impact utilisateur
3. **M√©triques** : RTO/RPO r√©els mesur√©s
4. **Analyse** : Points forts et points faibles de l'architecture
5. **Recommandations** : Am√©liorations possibles

---

## Livrables attendus

### ‚≠ê Livrables niveau 1 (obligatoires)

<CardGrid>
  <Card title="Analyse des SPOF" icon="warning">
    Identification des points de d√©faillance avec matrice de risques
  </Card>
  <Card title="Justification des choix" icon="pencil">
    Choix technologiques argument√©s (HAProxy, Keepalived, r√©plication...)
  </Card>
  <Card title="Sch√©ma d'architecture HA" icon="diagram">
    Architecture compl√®te avec tous les composants et flux
  </Card>
  <Card title="Plan d'adressage cluster" icon="table">
    Tableau IP complet de tous les n≈ìuds
  </Card>
  <Card title="Configuration HAProxy" icon="document">
    Fichier de config comment√© et expliqu√©
  </Card>
  <Card title="Tests de load balancing" icon="checkmark">
    Validation du fonctionnement et de la r√©partition
  </Card>
</CardGrid>

### ‚≠ê‚≠ê Livrables niveau 2

<CardGrid>
  <Card title="Configuration Keepalived" icon="network">
    IP virtuelle fonctionnelle avec tests de basculement
  </Card>
  <Card title="R√©plication MySQL" icon="database">
    Master-Slave configur√© et valid√©
  </Card>
  <Card title="Synchronisation fichiers" icon="folder">
    Solution d√©ploy√©e et test√©e (NFS ou Lsyncd)
  </Card>
  <Card title="Tests de panne" icon="scan">
    Sc√©narios de failover document√©s (web, HAProxy)
  </Card>
  <Card title="Proc√©dures de reprise" icon="open-book">
    Documentation pour promouvoir slave en master
  </Card>
</CardGrid>

### ‚≠ê‚≠ê‚≠ê Livrables niveau 3

<CardGrid>
  <Card title="Architecture sans SPOF" icon="template">
    Aucun point de d√©faillance unique
  </Card>
  <Card title="Master-Master MySQL" icon="database">
    R√©plication bidirectionnelle avec ProxySQL
  </Card>
  <Card title="Monitoring complet" icon="chart">
    Prometheus + Grafana avec dashboards et alertes
  </Card>
  <Card title="GlusterFS" icon="folder">
    Stockage r√©pliqu√© sans SPOF (optionnel)
  </Card>
  <Card title="Rapport de tests de charge" icon="document">
    Tests de r√©silience avec m√©triques RTO/RPO
  </Card>
  <Card title="Automatisation" icon="rocket">
    Scripts de d√©ploiement et de reprise automatiques
  </Card>
</CardGrid>

---

## Crit√®res d'√©valuation

| Crit√®re | ‚≠ê Niveau 1 | ‚≠ê‚≠ê Niveau 2 | ‚≠ê‚≠ê‚≠ê Niveau 3 |
|---------|------------|--------------|---------------|
| **Architecture** | Load balancing web fonctionnel | + IP virtuelle + r√©plication BDD | Architecture sans SPOF |
| **Disponibilit√©** | Service web redondant | RTO < 5 min, RPO < 5 min | RTO < 2 min, RPO < 1 min |
| **Basculement** | Manuel (red√©marrage service) | Automatique HAProxy, manuel BDD | Tout automatique |
| **Synchronisation** | Fichiers non trait√©s (acceptable) | Solution impl√©ment√©e (NFS/Lsyncd) | GlusterFS ou √©quivalent HA |
| **Tests** | Load balancing valid√© | Tests de panne web + HAProxy | Tests de charge + sc√©narios multiples |
| **Monitoring** | Logs consultables | Supervision de base | Prometheus/Grafana + alertes |
| **Documentation** | Sch√©ma + justifications | + Proc√©dures de reprise | Runbook complet + rapport tests |

:::tip[Conseils pour l'√©valuation]
- L'**analyse initiale** des SPOF et la **justification** des choix sont essentielles
- Les **tests de panne** doivent √™tre rigoureux et document√©s avec m√©triques
- La **qualit√© de la documentation** permet la reprise par un autre admin
- Le **rapport de tests** (‚≠ê‚≠ê‚≠ê) d√©montre la ma√Ætrise professionnelle
:::

---

## Ressources compl√©mentaires

- [Documentation HAProxy](http://www.haproxy.org/#docs)
- [Guide Keepalived](https://www.keepalived.org/documentation.html)
- [R√©plication MySQL](https://dev.mysql.com/doc/refman/8.0/en/replication.html)
- [GlusterFS Documentation](https://docs.gluster.org/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [GLPI HA Best Practices](https://glpi-project.org/) (forums communautaires)

:::tip[Prochaine √©tape]
Une fois cette mission termin√©e et valid√©e, vous pourrez passer √† la [Mission 6 - Services sur le cluster](/missions/mission-6) o√π vous ajouterez des services compl√©mentaires sur votre infrastructure haute disponibilit√©.
:::